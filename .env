# ============================================================
# Spec2Ship — environment configuration
# Copy to .env and adjust as needed.
# ============================================================

# --- Database & Redis ---
POSTGRES_USER=spec2ship
POSTGRES_PASSWORD=spec2ship
POSTGRES_DB=spec2ship
REDIS_URL=redis://redis:6379/0

# --- Ports ---
API_PORT=8000
WEB_PORT=3000

# --- Workspace paths (inside containers) ---
WORKSPACE_PATH=/workspace/sample_workspace
WORKSPACES_ROOT=/workspace/workspaces
ARTIFACTS_DIR=/data/artifacts

# --- Workspace isolation ---
ISOLATE_WORKSPACES=true
RUN_WORKSPACES_DIR=/data/run_workspaces

# --- Upload safety limits ---
WORKSPACE_UPLOAD_MAX_BYTES=209715200   # 200 MB
WORKSPACE_EXTRACT_MAX_BYTES=838860800  # 800 MB
WORKSPACE_EXTRACT_MAX_FILES=20000
WORKSPACE_EXTRACT_MAX_FILE_BYTES=52428800  # 50 MB

# --- Timeouts (seconds) ---
# LOW values below are suitable for local / slow machines.
# For a production server, increase as shown in the comments.

PREFLIGHT_SECONDS=30           # server: 30
SMOKE_SECONDS=60               # server: 120
APPLY_PATCH_SECONDS=120        # server: 180
MAX_COMMAND_SECONDS=300        # server: 900
GIT_COMMAND_SECONDS=120        # server: 600
TEST_COMMAND_SECONDS=300       # server: 1800
RQ_JOB_TIMEOUT_SECONDS=7200   # server: 21600
APPROVAL_WAIT_SECONDS=600

# --- Patch proposer ---
# "rules"  → deterministic offline (default, works on any machine)
# "ollama" → LLM via Ollama (needs docker-compose.llm.yml + model pulled)
# "hf"     → local HuggingFace model (needs docker-compose.train.yml)
PATCHER_MODE=rules

PATCH_MAX_ATTEMPTS=2
MAX_PATCH_ITERATIONS=2

# --- Ollama (only if PATCHER_MODE=ollama) ---
# Requires: docker compose -f docker-compose.yml -f docker-compose.llm.yml up
# Then: docker exec <ollama-container> ollama pull qwen2.5-coder:7b
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5-coder:7b
OLLAMA_TIMEOUT_SECONDS=180     # ultra-low CPU test profile (server: 600+)
OLLAMA_TEMPERATURE=0.0
OLLAMA_NUM_CTX=1024             # ultra-low CPU test profile (server: 8192+)

# --- Code context for LLM prompts ---
CODE_CONTEXT_MAX_FILES=8        # server: 12-15
CODE_CONTEXT_MAX_CHARS=12000    # server: 20000+

# --- HuggingFace local patcher (only if PATCHER_MODE=hf) ---
# Requires: docker-compose.train.yml
HF_MODEL=Qwen/Qwen2.5-Coder-0.5B-Instruct
HF_ADAPTER_PATH=
HF_DEVICE=cpu                   # server with GPU: cuda
HF_MAX_NEW_TOKENS=512           # server: 800+
HF_TEMPERATURE=0.2
HF_TOP_P=0.95

# --- ML scripts ---
ML_DIR=/ml

# --- SWE-bench defaults (LOW for flow testing) ---
# These are the preset values; override via ticket text for a specific run.
# For real evaluation on a server: limit=50-300, max_workers=4+, max_steps=200+
SWEBENCH_PROMPT_DATASET=princeton-nlp/SWE-bench_Lite_bm25_13K
SWEBENCH_DATASET_NAME=princeton-nlp/SWE-bench_Lite
SWEBENCH_MAX_WORKERS=1

# ============================================================
# SERVER TUNING GUIDE (uncomment and adjust for deployment)
# ============================================================
# MAX_COMMAND_SECONDS=900
# TEST_COMMAND_SECONDS=1800
# OLLAMA_TIMEOUT_SECONDS=900
# OLLAMA_NUM_CTX=8192
# HF_MAX_NEW_TOKENS=1000
# HF_DEVICE=cuda
# CODE_CONTEXT_MAX_FILES=15
# CODE_CONTEXT_MAX_CHARS=24000
# SWEBENCH_MAX_WORKERS=4
# RQ_JOB_TIMEOUT_SECONDS=21600
